# K8s 资源yaml配置

## Namespace

```yaml
kind: Namespace
apiVersion: v1
metadata:
  name: app
  annotations:
    desc: 应用
  labels:
    name: app
spec:
  finalizers:
    - kubernetes
status:
  phase: Active
```

## Pod

```yaml
apiVersion: v1 # 指定api版本, 必须在kubectl api-versions中有
kind: Pod # 指定创建的资源, 必须在kubectl api-resources
metadata: # 资源的元数据
  name: web # 资源的名字, 在同一个namespace中必须唯一
  namespace: default # 所属空间
  labels: # 标签
    web: postStart
spec: # 指该资源的内容
  # 重启策略, 默认Always, 容器退出后会立即创建一个相同的容器, 还有OnFailure,Never
  restartPolicy: Always
  affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - {{.Component}}
            # 这个定义标签的不同值的node, 属于同一位置, 一般使用node之间都唯一值的key, 特殊情况下才要用,相当于给node分组
            topologyKey: kubernetes.io/hostname
  containers:
  - name: tomcat # 容器名
    image: tomcat:8.5.41-jre8-alpine # 镜像
    # 镜像拉取策略# Always,每次都检查,  Never,每次都不检查（不管本地是否有）, IfNotPresent,如果本地有就不检查,如果没有就拉取
    imagePullPolicy: IfNotPresent
    #启动容器的运行命令,将覆盖容器中的Entrypoint,对应Dockefile中的ENTRYPOINT
    command: ['/bin/sh', "-c" ]
    args: ["${catalina_path} run"] #启动容器的命令参数,对应Dockerfile中CMD参数
    env: # 指定容器中的环境变量
    - name: catalina_path # 变量的名字
      value: "/usr/local/tomcat/bin/catalina.sh" #变量的值
    resources: # 资源管理
     # 容器运行时,最低资源需求,也就是说最少需要多少资源容器才能正常运行, k8s调度时只要node的资源会满足requests, 就有可能被调度到node上, 不填默认位limits
      requests:
        # CPU资源（核数）,两种方式,浮点数或者是整数+m,0.1=100m,最少值为0.001核（1m）, m表示千分之一的意思
        cpu: 0.1
        memory: 100Mi # 内存使用量
      limits: # 资源限制
        cpu: 0.5
        memory: 200Mi
    ports:
    - name: tomcat # 名称
      containerPort: 80 # 容器开放对外端口
      protocol: TCP # 端口协议
    lifecyle:
      preStop:
        exec:
          command: ['/usr/local/tomcat/bin/catalina.sh', 'stop']

  - name: nginx
    image: nginx:stable-alpine
    imagePullPolicy: IfNotPresent
    ports:
    - name: nginx
      containerPort: 80
    lifecycle: # 生命周期管理
      # 执行失败, pod会根据restartPolicy策略重启
      postStart: # 容器创建成功后立即运行
        exec:
          command: ["/bin/sh", "-c", "echo poststart >> /usr/share/nginx/html/index.html"]
      preStop: # 在容器被终止前的任务，用于优雅关闭应用程序、通知其他系统等等
        exec:
          # 先退出nignx
          command: ['/usr/local/nginx/sbin/nginx', '-s', 'quit']
    livenessProbe: #成功,设置容器 Ready 状态为 true
      httpGet: # 通过httpget检查健康,返回200-399之间,则认为容器正常
        # 这边可以直接使用spec.containers.ports.name
        port: nginx
        path: /index.html # URI
        # host: 127.0.0.1 # 主机
        scheme: HTTP # 协议
      initialDelaySeconds: 1 # 表明第一次检测在容器启动后多长时间后开始
      periodSeconds: 3 # 检查间隔时间
      timeoutSeconds: 5 # 检测的超时时间
      #也可以用这种方法
      #exec: 执行命令的方法进行监测,如果其退出码不为0,则认为容器正常
      #  command:
      #    - cat
      #    - /tmp/health
      #也可以用这种方法
      #tcpSocket: //通过tcpSocket检查健康
      #  port: number
    volumeMounts:  # 挂在在容器的pod卷
    - name: volume # 挂载设备的名字,与volumes[*].name 需要对应
      mountPath: /data # 挂载到容器的某个路径下
      readOnly: True
  nodeSelector: # 节点选择器
    # 部署在有标签service=rest的node上
    service: rest
  # 通过节点名字选择  
  # nodeName: node-1
  volumes: # 定义一组挂载设备
  - name: volume # 定义一个挂载设备的名字
    # emptyDir: {}
    hostPath:
      path: /opt # 挂载设备类型为hostPath,路径为宿主机下的/opt,这里设备类型支持很多种
```

## Replicaset

```yaml
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: replicaset-nginx
  labels:
    app: guestbook
    tier: backend
spec:
  replicas: 3
  selector:
    matchLabels:
      # 以下的labels不能重复, 如果有其他pod使用这两个标签会被ReplicaSet识别到
      app: guestbook
      tier: backend
  template: Pod Object
```

## Deployment

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deploy
  namespace: app
spec:
  replicas: 5
  selector:
    matchLabels:
    # 必填, 必须与以下template下metadata.labels的一致
    # 例子
    # "release" : "stable", "release" : "canary"
    # "environment" : "dev", "environment" : "qa", "environment" : "production"
    # "tier" : "frontend", "tier" : "backend", "tier" : "cache"
    # "partition" : "customerA", "partition" : "customerB"
     # "track" : "daily", "track" : "weekly"
      app: nginx
      release: stable
      tier: slb
      partition: website
  # 滚动升级时，容器准备就绪时间, 这段时间会堵塞, 停止更新
  minReadySeconds: 2
  # 控制保存历史版本的数量, 历史版本可用于回滚，要记录历史版本需要在创建Deployment时使用 --record选项
  # kubectl apply -f nginx.yaml --record
  revisionHistoryLimit: 3
  strategy:
    # 默认RollingUpdate: 滚动更新, 还可以是Recreate: 先删除让后重建
    type: "RollingUpdate"
    ## maxSurge, maxUnavailable 都可以是0值, 但是不能同时设置为0
    # 以 replicas: 5, maxSurge: 1, maxUnavailable: 1 为例
    # 先会创建一个pod, 存活6个, 然后终止两个存, 活4个, 然后再创建两个, 然后在启动两个, 存活6g个, 在终止两个, 知道旧的pod全部更新完成, 存活的数量最少4个, 最多6个
    # 如果replicas=maxSurge, 相当于蓝绿发布
    rollingUpdate:
      # 浪涌, 最多额外的pod数量, 可以是数量也可以是百分比, 100%会立即启动总数量的pod
      maxSurge: 1
      # 可以是数值也可以是百分比, 最多不可用pod数量, 为0 表示在pod启动就绪之前不要关闭旧pod
      maxUnavailable: 0
  template: Pod Object
```
